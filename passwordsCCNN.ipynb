{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook illustrates password case \n",
    "# unpack prepared combinatorics dataset from passwords.zip in the directory containing this\n",
    "# notebook\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "partition = {}\n",
    "labels = {}\n",
    "\n",
    "goods = []\n",
    "bads = []\n",
    "\n",
    "for line in open('goodpasswords.txt'):\n",
    "    word =line.strip().split(',')[0]\n",
    "    goods.append('psw'+word+'.npz')\n",
    "    \n",
    "for line in open('badpasswords.txt'):\n",
    "    word =line.strip().split(',')[0]\n",
    "    bads.append('psw'+word+'.npz')\n",
    "\n",
    "\n",
    "base_dir = './passwords'\n",
    "try:\n",
    "    os.mkdir(base_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "\n",
    "try:\n",
    "    os.mkdir(train_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_goods_dir = os.path.join(train_dir, 'goods')\n",
    "try:\n",
    "    os.mkdir(train_goods_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "partition['train'] = goods[:1000]\n",
    "\n",
    "\n",
    "for fname in goods[:1000]:\n",
    "    labels[fname] = 1\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(train_goods_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "train_bads_dir = os.path.join(train_dir, 'bads')\n",
    "try:\n",
    "    os.mkdir(train_bads_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "partition['train'] += bads[:1000]\n",
    "\n",
    "\n",
    "for fname in bads[:1000]:\n",
    "    labels[fname] = 0\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(train_bads_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "\n",
    "try:\n",
    "    os.mkdir(test_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "test_goods_dir = os.path.join(test_dir, 'goods')\n",
    "try:\n",
    "    os.mkdir(test_goods_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "partition['test'] = goods[1000:1500]\n",
    "\n",
    "\n",
    "for fname in goods[1000:1500]:\n",
    "    labels[fname] = 1\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(test_goods_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "test_bads_dir = os.path.join(test_dir, 'bads')\n",
    "try:\n",
    "    os.mkdir(test_bads_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "partition['test'] += bads[1000:1500]\n",
    "\n",
    "\n",
    "for fname in bads[1000:1500]:\n",
    "    labels[fname] = 0\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(test_bads_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "valid_dir = os.path.join(base_dir,'validate')\n",
    "\n",
    "try:\n",
    "    os.mkdir(valid_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "valid_goods_dir = os.path.join(valid_dir, 'goods')\n",
    "try:\n",
    "    os.mkdir(valid_goods_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "partition['validate'] = goods[1500:]\n",
    "\n",
    "\n",
    "for fname in goods[1500:]:\n",
    "    labels[fname] = 1\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(valid_goods_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "valid_bads_dir = os.path.join(valid_dir, 'bads')\n",
    "try:\n",
    "    os.mkdir(valid_bads_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "partition['validate'] += bads[1500:]\n",
    "\n",
    "\n",
    "for fname in bads[1500:]:\n",
    "    labels[fname] = 0\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(valid_bads_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "class DataGenerator(keras.utils.all_utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(210,210), n_channels=210,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            if self.labels[ID] == 1:\n",
    "                src = os.path.join(train_goods_dir, str(ID))\n",
    "            else:\n",
    "                src = os.path.join(train_bads_dir, str(ID))\n",
    "            X[i,] = np.load(src)['arr_0']\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "class ValidateGenerator(keras.utils.all_utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(210,210), n_channels=210,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            if self.labels[ID] == 1:\n",
    "                src = os.path.join(valid_goods_dir, str(ID))\n",
    "            else:\n",
    "                src = os.path.join(valid_bads_dir, str(ID))\n",
    "            X[i,] = np.load(src)['arr_0']\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels)\n",
    "validation_generator = ValidateGenerator(partition['validate'], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape = (210,210,210)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(8, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "             metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    training_generator,\n",
    "    steps_per_epoch=30,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "validation_steps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
