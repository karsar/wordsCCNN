{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook illustrates palindrome case \n",
    "# unpack prepared combinatorics dataset from palindromes.zip in the directory containing this\n",
    "# notebook\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "partition = {}\n",
    "labels = {}\n",
    "\n",
    "base_dir = './palindromes'\n",
    "try:\n",
    "    os.mkdir(base_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "\n",
    "try:\n",
    "    os.mkdir(train_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_palindromes_dir = os.path.join(train_dir, 'palindromes')\n",
    "try:\n",
    "    os.mkdir(train_palindromes_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fnames = ['pdata{}.npz'.format(i) for i in range(1000)]\n",
    "partition['train'] = fnames\n",
    "\n",
    "\n",
    "for fname in fnames:\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(train_palindromes_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "train_nonpalindromes_dir = os.path.join(train_dir, 'nonpalindromes')\n",
    "try:\n",
    "    os.mkdir(train_nonpalindromes_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "fnames = ['data{}.npz'.format(i) for i in range(1000)]\n",
    "\n",
    "partition['train'] += fnames\n",
    "\n",
    "\n",
    "for fname in fnames:\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(train_nonpalindromes_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "\n",
    "try:\n",
    "    os.mkdir(test_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "test_palindromes_dir = os.path.join(test_dir, 'palindromes')\n",
    "try:\n",
    "    os.mkdir(test_palindromes_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fnames = ['pdata{}.npz'.format(i) for i in range(1000,1500)]\n",
    "partition['test'] = fnames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for fname in fnames:\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(test_palindromes_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "test_nonpalindromes_dir = os.path.join(test_dir, 'nonpalindromes')\n",
    "try:\n",
    "    os.mkdir(test_nonpalindromes_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fnames = ['data{}.npz'.format(i) for i in range(1000,1500)]\n",
    "partition['test'] += fnames\n",
    "\n",
    "\n",
    "for fname in fnames:\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(test_nonpalindromes_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "valid_dir = os.path.join(base_dir,'validate')\n",
    "\n",
    "try:\n",
    "    os.mkdir(valid_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "valid_palindromes_dir = os.path.join(valid_dir, 'palindromes')\n",
    "try:\n",
    "    os.mkdir(valid_palindromes_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fnames = ['pdata{}.npz'.format(i) for i in range(1500,2000)]\n",
    "partition['validate'] = fnames\n",
    "\n",
    "\n",
    "for fname in fnames:\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(valid_palindromes_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "valid_nonpalindromes_dir = os.path.join(valid_dir, 'nonpalindromes')\n",
    "try:\n",
    "    os.mkdir(valid_nonpalindromes_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fnames = ['data{}.npz'.format(i) for i in range(1500,2000)]\n",
    "partition['validate'] += fnames\n",
    "\n",
    "\n",
    "for fname in fnames:\n",
    "    src = os.path.join('./', fname)\n",
    "    dst = os.path.join(valid_nonpalindromes_dir, fname)\n",
    "    shutil.copyfile(src, dst) \n",
    "    \n",
    "\n",
    "labels = {}\n",
    "for part in partition:\n",
    "    for el in partition[part]:\n",
    "        if 'pdata' in el:\n",
    "            labels[el] = 1\n",
    "        else:\n",
    "            labels[el] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "class DataGenerator(keras.utils.all_utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(210,210), n_channels=210,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            if 'p' == str(ID)[0]:\n",
    "                src = os.path.join(train_palindromes_dir, str(ID))\n",
    "            else:\n",
    "                src = os.path.join(train_nonpalindromes_dir, str(ID))\n",
    "            X[i,] = np.load(src)['arr_0']\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "class ValidateGenerator(keras.utils.all_utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(210,210), n_channels=210,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            if 'p' == str(ID)[0]:\n",
    "                src = os.path.join(valid_palindromes_dir, str(ID))\n",
    "            else:\n",
    "                src = os.path.join(valid_nonpalindromes_dir, str(ID))\n",
    "            X[i,] = np.load(src)['arr_0']\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels)\n",
    "validation_generator = ValidateGenerator(partition['validate'], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape = (210,210,210)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(8, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    training_generator,\n",
    "    steps_per_epoch=30,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
