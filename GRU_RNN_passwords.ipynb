{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aZeXM-8Pp9udYLLrIgxh7mDKgD-5QYJY","timestamp":1704953214327}],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyPpcuGHWE31eQkrUkoWPL9V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IMNFUVI1e_pd"},"outputs":[],"source":["import keras\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, GRU, Dense\n","import pandas as pd\n","import random\n","import string\n","import numpy as np"]},{"cell_type":"code","source":["words = []\n","labels = []\n","\n","for line in open('goodpasswords.txt'):\n","   words.append(line.split(',')[0])\n","   labels.append(1)\n","\n","for line in open('badpasswords.txt'):\n","   words.append(line.split(',')[0])\n","   labels.append(0)\n"],"metadata":{"id":"C0DP-BBSfLvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def replace2(word):\n","  w = ''\n","  for el in word:\n","    if el == 'A':\n","      w+='#'\n","    elif el ==\"H\":\n","      w+='?'\n","    else:\n","      w+=el\n","  return w\n","\n","def replace1(word):\n","  w = ''\n","  for el in word:\n","    if el == 'A':\n","      w+='#'\n","    else:\n","      w+=el\n","  return w"],"metadata":{"id":"FxIH9f0lNAlB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_accuracies = []\n","test_accuracies = []\n","test1_accuracies = []\n","test2_accuracies = []\n","\n","for seed in range(200):\n","\n","\n","    random.seed(seed)\n","    combined_data = list(zip(words, labels))\n","    random.shuffle(combined_data)\n","    words, labels = zip(*combined_data)\n","\n","    train_words = words[:1000]\n","    train_labels = labels[:1000]\n","\n","    test_words = words[1000:1500]\n","    test_labels = labels[1000:1500]\n","\n","    valid_words = words[1500:]\n","    valid_labels = labels[1500:]\n","\n","    max_word_length = max(len(word) for word in train_words)\n","    char_tokens = [list(word) for word in train_words]\n","\n","    # Create a vocabulary of unique characters\n","    char_set = set([char for word in char_tokens for char in word]+['#','?'])\n","    num_chars = len(char_set)\n","\n","    # Create a dictionary to map characters to indices\n","    char_indices = {char: i + 1 for i, char in enumerate(char_set)}\n","    indices_char = {i + 1: char for i, char in enumerate(char_set)}\n","\n","    # Convert words to sequences of character indices\n","    sequences = [[char_indices[char] for char in word] for word in char_tokens]\n","\n","    # Pad sequences to ensure equal length\n","    padded_sequences = pad_sequences(sequences, maxlen=max_word_length)\n","\n","    # Convert labels to categorical\n","    num_classes = len(set(train_labels))\n","    categorical_labels = keras.utils.to_categorical(train_labels, num_classes)\n","\n","    # Build the RNN GRU model\n","    model = Sequential()\n","    model.add(Embedding(input_dim=num_chars + 1, output_dim=50, input_length=max_word_length))\n","    model.add(GRU(100))\n","    model.add(Dense(20, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    # Train the model\n","    hist = model.fit(padded_sequences, categorical_labels, epochs=100, batch_size=32, validation_split=0.2)\n","    val_accuracies.append(hist.history['val_accuracy'][-1])\n","    test_sequences = [[char_indices[char] for char in word] for word in test_words]\n","    padded_test_sequences = pad_sequences(test_sequences, maxlen=max_word_length)\n","\n","    # Convert labels to categorical\n","    categorical_test_labels = keras.utils.to_categorical(test_labels, num_classes)\n","\n","    # Evaluate the model on the test data\n","    loss, accuracy = model.evaluate(padded_test_sequences, categorical_test_labels)\n","    test_accuracies.append(accuracy)\n","    print(f'Test Loss: {loss:.4f}')\n","    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","\n","    test1_words = [replace1(wd)for wd in test_words]\n","\n","    diff = 0\n","    letters = 0\n","    for i in range(len(test_words)):\n","       diff+= sum(l1 != l2 for l1, l2 in zip(test1_words[i],test_words[i]))\n","       letters+=len(test_words[i])\n","\n","    print (diff)\n","    print (diff*100.0/letters)\n","    test_sequences = [[char_indices[char] for char in word] for word in test1_words]\n","    padded_test_sequences = pad_sequences(test_sequences, maxlen=max_word_length)\n","\n","    # Convert labels to categorical\n","    categorical_test_labels = keras.utils.to_categorical(test_labels, num_classes)\n","\n","    # Evaluate the model on the test data\n","    loss, accuracy = model.evaluate(padded_test_sequences, categorical_test_labels)\n","    test1_accuracies.append(accuracy)\n","    print(f'Test Loss: {loss:.4f}')\n","    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n","\n","    test1_words = [replace2(wd)for wd in test_words]\n","\n","    diff = 0\n","    letters = 0\n","    for i in range(len(test_words)):\n","       diff+= sum(l1 != l2 for l1, l2 in zip(test1_words[i],test_words[i]))\n","       letters+=len(test_words[i])\n","\n","    print (diff)\n","    print (diff*100.0/letters)\n","    test_sequences = [[char_indices[char] for char in word] for word in test1_words]\n","    padded_test_sequences = pad_sequences(test_sequences, maxlen=max_word_length)\n","\n","    # Convert labels to categorical\n","    categorical_test_labels = keras.utils.to_categorical(test_labels, num_classes)\n","\n","    # Evaluate the model on the test data\n","    loss, accuracy = model.evaluate(padded_test_sequences, categorical_test_labels)\n","    test2_accuracies.append(accuracy)\n","    print(f'Test Loss: {loss:.4f}')\n","    print(f'Test Accuracy: {accuracy * 100:.2f}%')"],"metadata":{"id":"zrSx-lgMNJxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print (val_accuracies)\n","print ('mean',np.mean(val_accuracies))\n","print ('std',np.std(val_accuracies))\n","\n","print ('test accuracies')\n","print (test_accuracies)\n","print ('mean',np.mean(test_accuracies))\n","print ('std',np.std(test_accuracies))\n","\n","print ('one letter replaced test accuracies')\n","print (test1_accuracies)\n","print ('mean',np.mean(test1_accuracies))\n","print ('std',np.std(test1_accuracies))\n","\n","print ('two letter replaced test accuracies')\n","print (test2_accuracies)\n","print ('mean',np.mean(test2_accuracies))\n","print ('std',np.std(test2_accuracies))"],"metadata":{"id":"go6PkqWjNVFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# testing whether accuracies for the nonpermuted and permuted datasets are different\n","from scipy import stats\n","t, pval = stats.ttest_ind(test_accuracies, test1_accuracies)\n","print(t)\n","print(pval)"],"metadata":{"id":"RzdqzMEWY_34"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# testing whether accuracies for the nonpermuted and permuted datasets are different\n","from scipy import stats\n","t, pval = stats.ttest_ind(test_accuracies, test2_accuracies)\n","print(t)\n","print(pval)"],"metadata":{"id":"3JiW6nDCZFLZ"},"execution_count":null,"outputs":[]}]}